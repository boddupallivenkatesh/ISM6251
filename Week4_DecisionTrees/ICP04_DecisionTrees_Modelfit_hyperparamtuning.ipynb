{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f52619b",
   "metadata": {},
   "source": [
    "# 1. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "699b8189",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75404e0c",
   "metadata": {},
   "source": [
    "# 2. Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c3846553",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('airbnb_train_X_price_gte_150.csv') \n",
    "y_train = pd.read_csv('airbnb_train_y_price_gte_150.csv') \n",
    "X_test = pd.read_csv('airbnb_test_X_price_gte_150.csv') \n",
    "y_test = pd.read_csv('airbnb_test_y_price_gte_150.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e5951b",
   "metadata": {},
   "source": [
    "# 3. Modeling the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "27a8fd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = pd.DataFrame({\"model\": [], \"Accuracy\": [], \"Precision\": [], \"Recall\": [], \"F1\": []})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efee309",
   "metadata": {},
   "source": [
    "# 4. Decision tree model using the randomsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b63c5dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 500 candidates, totalling 2500 fits\n",
      "The best precision score is 0.858619053624482\n",
      "... with parameters: {'min_samples_split': 46, 'min_samples_leaf': 16, 'min_impurity_decrease': 0.0031, 'max_leaf_nodes': 143, 'max_depth': 27, 'criterion': 'entropy'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vvenk\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "5 fits failed out of a total of 2500.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\vvenk\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\vvenk\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 937, in fit\n",
      "    super().fit(\n",
      "  File \"C:\\Users\\vvenk\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 250, in fit\n",
      "    raise ValueError(\n",
      "ValueError: min_samples_split must be an integer greater than 1 or a float in (0.0, 1.0]; got the integer 1\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\vvenk\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [0.82488631 0.77844887 0.84756926 0.83234847 0.82627955 0.82488631\n",
      " 0.82487201 0.82487201 0.84428996 0.82488631 0.82624653 0.82487201\n",
      " 0.82488631 0.82566132 0.84221652 0.82488631 0.84189061 0.77844887\n",
      " 0.82488631 0.84499722 0.83942426 0.82051074 0.83700681 0.83581309\n",
      " 0.84319314 0.82488631 0.84740936 0.85068392 0.82488631 0.83304336\n",
      " 0.82855513 0.82586846 0.82856943 0.83556602 0.81962525 0.77844887\n",
      " 0.8435269  0.83247073 0.83581309 0.84805907 0.82488631 0.82488631\n",
      " 0.8389098  0.84805907 0.82488631 0.83024761 0.82779601 0.8418381\n",
      " 0.83416047 0.83109745 0.82487201 0.830101   0.82488631 0.82514459\n",
      " 0.82488631 0.82488631 0.84544946 0.8282673  0.83356477 0.83026191\n",
      " 0.83170101 0.84250571 0.83556602 0.83552884 0.82488631 0.82487201\n",
      " 0.84312115 0.82488631 0.82586846 0.82488631 0.82498315 0.84912983\n",
      " 0.84062648 0.82514459 0.77844887 0.8299782  0.82926209 0.8280068\n",
      " 0.82627955 0.77844887 0.77844887 0.82837336 0.84740936 0.8354221\n",
      " 0.8539747  0.82488631 0.77844887 0.84287178 0.84818348 0.83548537\n",
      " 0.84526971 0.83118427 0.82488631 0.8278815  0.84346414 0.8278815\n",
      " 0.83797578 0.83944799 0.83942426 0.84828979 0.82488631 0.83024761\n",
      " 0.82627955 0.82488631 0.82488631 0.8360193  0.84708677 0.77844887\n",
      " 0.8311907  0.8237614  0.83231997 0.83304336 0.82627955 0.82627955\n",
      " 0.82487201 0.84499722 0.85358106 0.83247073 0.82627955 0.82514459\n",
      " 0.83460008 0.83433522 0.83997506 0.77844887 0.82488631 0.8317569\n",
      " 0.82855513 0.83238171 0.82926049 0.82488631 0.82488631 0.83231997\n",
      " 0.82929658 0.83024761 0.83556602 0.83433522 0.82487201 0.82488631\n",
      " 0.83581309 0.82514459 0.84041874 0.82488631 0.82578511 0.83617917\n",
      " 0.8280068  0.8409476  0.82488631 0.84062648 0.82488631 0.82488631\n",
      " 0.84276655 0.83302929 0.83581309 0.83304336 0.84189061 0.8317569\n",
      " 0.83997506 0.84679758 0.82488631 0.82488631 0.82488631 0.83453927\n",
      " 0.84289846 0.82488631 0.82841043 0.8567996  0.84649426 0.83965785\n",
      " 0.83525437 0.82841455 0.82978518 0.85346728 0.8317569  0.83406779\n",
      " 0.84356026 0.83581309 0.83581309 0.82488631 0.83581309 0.84153589\n",
      " 0.8227119  0.82488631 0.82488631 0.82627955 0.82926049 0.8461239\n",
      " 0.82488631 0.82488631 0.82488631 0.84533717 0.82487201 0.82488631\n",
      " 0.83364647 0.83663271 0.83247073 0.82627955 0.82488631 0.8278815\n",
      " 0.83902338 0.85276431 0.83304336 0.84912338 0.82627955 0.82514459\n",
      " 0.83877009 0.83581338 0.83189672 0.82514459 0.82447048 0.82531526\n",
      " 0.830101   0.82488631 0.82488631 0.84593639 0.8325115  0.84491204\n",
      " 0.83304336 0.84499722 0.82488631 0.83581309 0.83590745 0.84475804\n",
      " 0.82488631 0.82926049 0.77844887 0.82488631 0.83762658 0.84276655\n",
      " 0.83442392 0.830101   0.82627955 0.83711106 0.83581309 0.82509452\n",
      " 0.82488631 0.82488631 0.83986661 0.83127438 0.84384569 0.82488631\n",
      " 0.8409476  0.82488631 0.83480762 0.82488631 0.82488631 0.77844887\n",
      " 0.82514459 0.84649426 0.8280068  0.84649426 0.82970101 0.8280068\n",
      " 0.82488631 0.82985596 0.8280068  0.83752591 0.83737937 0.82488631\n",
      " 0.83609512 0.83956    0.82599224 0.84379621 0.82970101 0.77844887\n",
      " 0.830101   0.8398029  0.84721677 0.8239222  0.82929658 0.83898786\n",
      " 0.83035102 0.82514459 0.83672269 0.83854826 0.84133442 0.84291253\n",
      " 0.82627955 0.8360193  0.83247073 0.83151218 0.82514459 0.82961356\n",
      " 0.83571509 0.83581309 0.83966532 0.8347129  0.84360423 0.8280068\n",
      " 0.83267695 0.83397462 0.77844887 0.84913799 0.83280977 0.83304336\n",
      " 0.82488631 0.82488631 0.84409773 0.84880083 0.83099226 0.8409476\n",
      " 0.82514459 0.82488631 0.85861905 0.8280068  0.82488631 0.82488631\n",
      " 0.77844887 0.83954218 0.84786998 0.8342809  0.82567763 0.83589091\n",
      " 0.83002511 0.82937615 0.82841043 0.82488631 0.82488631 0.82488631\n",
      " 0.82487201 0.83575174 0.82488631 0.82488631 0.84901565 0.82488631\n",
      " 0.8490245  0.82488631 0.84191732 0.84283953 0.82834937 0.84133442\n",
      " 0.82514459 0.84087839 0.8519014  0.85063152 0.83304336 0.84012145\n",
      " 0.82488631 0.84649426 0.84673915 0.82978518 0.84708677 0.85060345\n",
      " 0.82488631 0.85700343 0.84420218 0.8317569  0.8280068  0.8278815\n",
      " 0.83581309 0.8278815  0.84649044 0.83304336 0.82488631 0.82488631\n",
      " 0.83460008 0.82567763 0.84280804 0.77844887 0.82936558 0.83511415\n",
      " 0.83861123 0.83949409 0.82514459 0.82627955 0.82904928 0.83997506\n",
      " 0.77844887 0.83331313 0.82841455 0.83942419 0.83215973 0.82193826\n",
      " 0.82926049 0.82488631 0.83449872 0.84132283 0.82488631 0.83133873\n",
      " 0.82488631 0.82488631 0.82488631 0.83302929 0.82487201 0.8317569\n",
      " 0.83012345 0.82488631 0.82488631 0.82488631 0.83194758 0.84428996\n",
      " 0.82586846 0.82514459 0.84381077 0.82841043 0.83863819 0.77844887\n",
      " 0.83304336 0.83042632 0.84686788 0.84494157 0.82861689 0.82614502\n",
      " 0.82488631 0.84156968 0.83382801 0.8325115  0.83263835 0.82841043\n",
      " 0.83987605 0.82488631 0.82488631 0.85020423 0.82488631 0.8278815\n",
      " 0.83581309 0.82488631 0.83153266 0.84438497 0.8347129  0.82488631\n",
      " 0.83536416 0.83834604 0.83302929 0.84491204 0.84098194 0.84143592\n",
      " 0.8280068  0.83762658 0.84409773 0.84530705 0.82514459 0.83302929\n",
      " 0.83517315 0.82937615 0.82509452 0.82488631 0.82488631 0.82957319\n",
      " 0.85202813 0.84288759 0.82488631 0.82586846 0.84035934 0.82488631\n",
      " 0.82488631 0.82488631 0.85207368 0.84501705 0.83886117 0.82488631\n",
      " 0.82488631 0.84491204 0.84134066 0.83669304 0.84478834 0.84221652\n",
      " 0.82488631 0.77844887 0.83231997 0.82904928 0.84835738 0.83443789\n",
      " 0.82488631 0.83783511 0.82488631 0.837358   0.83825509 0.84382474\n",
      " 0.83453927 0.8249603  0.82488631 0.82881207 0.8435269  0.83304336\n",
      " 0.83581309 0.83490783 0.83311633 0.84108955 0.84123286 0.82488631\n",
      " 0.8389098  0.82575817 0.82856943 0.82488631 0.8389098  0.82932352\n",
      " 0.83247073 0.83304336 0.82586846        nan 0.82904928 0.8429271\n",
      " 0.83917863 0.83760339 0.83348011 0.82488631 0.827423   0.83556484\n",
      " 0.83308583 0.83781623]\n",
      "  warnings.warn(\n",
      "C:\\Users\\vvenk\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the train scores are non-finite: [0.82583601 0.77774902 0.86164054 0.83774958 0.83082791 0.82583601\n",
      " 0.82811686 0.82814917 0.84936169 0.82583601 0.84944995 0.82811686\n",
      " 0.82583601 0.82799056 0.84752902 0.82583601 0.84767897 0.77774902\n",
      " 0.82583601 0.85228415 0.85623245 0.90921703 0.85571576 0.84238417\n",
      " 0.88228095 0.82583601 0.85400728 0.87634088 0.82583601 0.84049152\n",
      " 0.83219021 0.83087084 0.82973987 0.8428255  0.88424105 0.77774902\n",
      " 0.85286529 0.83861476 0.84238417 0.85843965 0.82583601 0.82583601\n",
      " 0.84840174 0.85849603 0.82583601 0.83524992 0.85189012 0.84834074\n",
      " 0.87069688 0.83761552 0.82814917 0.83542494 0.82583601 0.82851932\n",
      " 0.82583601 0.82583601 0.85774412 0.85374536 0.86156909 0.83278796\n",
      " 0.83765143 0.84772977 0.8428255  0.84314121 0.82583601 0.82811686\n",
      " 0.85713699 0.82583601 0.83024509 0.82583601 0.82808965 0.88852911\n",
      " 0.84469235 0.82851932 0.77774902 0.83580752 0.85416447 0.83272552\n",
      " 0.83082791 0.77774902 0.77774902 0.85379967 0.85400728 0.84476624\n",
      " 0.87471552 0.82583601 0.77774902 0.85547508 0.88142098 0.84597667\n",
      " 0.87368907 0.8367705  0.82583601 0.83301112 0.85583668 0.83301112\n",
      " 0.84954155 0.86417604 0.85623245 0.87044756 0.82583601 0.83506881\n",
      " 0.83082791 0.82583601 0.82583601 0.84408099 0.85516646 0.77774902\n",
      " 0.86223461 0.889348   0.8394209  0.84049152 0.83082791 0.83082791\n",
      " 0.82814917 0.85228415 0.87379127 0.83861476 0.83082791 0.82851932\n",
      " 0.84821528 0.83784463 0.84827272 0.77774902 0.82583601 0.83864725\n",
      " 0.83219021 0.85694095 0.83452251 0.82583601 0.82583601 0.8394209\n",
      " 0.8343879  0.83506881 0.8428255  0.83790805 0.82814917 0.82583601\n",
      " 0.84238417 0.82851932 0.85306461 0.82583601 0.82845794 0.84412918\n",
      " 0.83272552 0.84559228 0.82583601 0.84469235 0.82583601 0.82583601\n",
      " 0.84810619 0.83941205 0.84238417 0.84049152 0.84751659 0.83864725\n",
      " 0.84827272 0.85496571 0.82583601 0.82583601 0.82583601 0.85176411\n",
      " 0.86116503 0.82583601 0.83235552 0.88123384 0.85480087 0.86093392\n",
      " 0.85970014 0.83487784 0.85375413 0.87288293 0.83864725 0.84047399\n",
      " 0.8973742  0.84238417 0.84238417 0.82583601 0.84238417 0.85081328\n",
      " 0.84634984 0.82583601 0.82583601 0.83082791 0.83397202 0.86735948\n",
      " 0.82583601 0.82583601 0.82583601 0.88754812 0.82814917 0.82583601\n",
      " 0.83971985 0.84430946 0.83861476 0.83082791 0.82583601 0.83301112\n",
      " 0.87632609 0.87285525 0.84049152 0.86662284 0.83082791 0.82851932\n",
      " 0.86096842 0.86594813 0.86105736 0.82851932 0.85440082 0.84695533\n",
      " 0.83542494 0.82583601 0.82583601 0.85396128 0.83800353 0.85280786\n",
      " 0.84049152 0.85228415 0.82583601 0.84238417 0.85797302 0.85360999\n",
      " 0.82583601 0.83452251 0.77774902 0.82583601 0.84496355 0.84810619\n",
      " 0.86736255 0.83542494 0.83082791 0.84590947 0.84238417 0.84688781\n",
      " 0.82583601 0.82583601 0.86038795 0.83421932 0.85770617 0.82583601\n",
      " 0.84559228 0.82583601 0.84214072 0.82583601 0.82583601 0.77774902\n",
      " 0.82851932 0.85515207 0.83272552 0.85498969 0.83672211 0.83272552\n",
      " 0.82583601 0.85609445 0.83272552 0.86190533 0.86065629 0.82583601\n",
      " 0.84524616 0.85571576 0.83077589 0.86474157 0.83672211 0.77774902\n",
      " 0.83542494 0.86284149 0.85605656 0.89036936 0.8343879  0.85634141\n",
      " 0.83667334 0.82851932 0.8596592  0.84374718 0.84540251 0.85455366\n",
      " 0.83082791 0.84408099 0.83861476 0.83903085 0.82851932 0.85854697\n",
      " 0.86082638 0.84238417 0.86545901 0.83801796 0.86215311 0.83272552\n",
      " 0.84031158 0.85874882 0.77774902 0.86190135 0.85547621 0.84049152\n",
      " 0.82583601 0.82583601 0.8558059  0.87517374 0.85112061 0.84559228\n",
      " 0.82851932 0.82583601 0.87873732 0.83272552 0.82583601 0.82583601\n",
      " 0.77774902 0.84732028 0.86902294 0.84385511 0.83068731 0.85435129\n",
      " 0.83676151 0.83455062 0.83235552 0.82583601 0.82583601 0.82583601\n",
      " 0.82814917 0.8549621  0.82583601 0.82583601 0.86863065 0.82583601\n",
      " 0.86783596 0.82583601 0.84983447 0.8644927  0.85242243 0.84540251\n",
      " 0.82851932 0.84764319 0.89296185 0.86524418 0.84049152 0.86620304\n",
      " 0.82583601 0.85480087 0.85676346 0.85375413 0.85516646 0.88965547\n",
      " 0.82583601 0.87220492 0.90211713 0.83864725 0.83272552 0.83301112\n",
      " 0.84238417 0.83301112 0.85748347 0.84049152 0.82583601 0.82583601\n",
      " 0.84821528 0.83068731 0.85655152 0.77774902 0.85179997 0.84337684\n",
      " 0.84625332 0.87044305 0.82851932 0.83082791 0.83272057 0.84827272\n",
      " 0.77774902 0.85765731 0.83487784 0.84433678 0.83764329 0.94478819\n",
      " 0.83397202 0.82583601 0.86767214 0.85315165 0.82583601 0.83634263\n",
      " 0.82583601 0.82583601 0.82583601 0.83954919 0.82811686 0.83864725\n",
      " 0.83384655 0.82583601 0.82583601 0.82583601 0.86588353 0.84936169\n",
      " 0.83030851 0.82851932 0.85694604 0.83235552 0.85634039 0.77774902\n",
      " 0.84049152 0.85112061 0.86138449 0.85318962 0.85528267 0.8462253\n",
      " 0.82583601 0.84903845 0.85284254 0.83800353 0.83877395 0.83235552\n",
      " 0.84373893 0.82583601 0.82583601 0.86700072 0.82583601 0.83301112\n",
      " 0.84238417 0.82583601 0.83690262 0.87923804 0.83801796 0.82583601\n",
      " 0.85778507 0.84634707 0.83941205 0.85280786 0.86838664 0.85178595\n",
      " 0.83272552 0.84496355 0.8558059  0.86817503 0.82851932 0.83954919\n",
      " 0.85745189 0.83455062 0.84688781 0.82583601 0.82583601 0.84882477\n",
      " 0.87100595 0.87488674 0.82583601 0.83080794 0.85420662 0.82583601\n",
      " 0.82583601 0.82583601 0.86728668 0.85475218 0.86353019 0.82583601\n",
      " 0.82583601 0.85280786 0.88189725 0.84554907 0.86366758 0.84752902\n",
      " 0.82583601 0.77774902 0.8394209  0.83272057 0.8668435  0.8595431\n",
      " 0.82583601 0.85602064 0.82583601 0.84833116 0.84786933 0.8522297\n",
      " 0.85176411 0.8704339  0.82583601 0.85762767 0.85286529 0.84049152\n",
      " 0.84238417 0.89839513 0.87111776 0.85983033 0.85988158 0.82583601\n",
      " 0.84840174 0.83018155 0.82973987 0.82583601 0.84840174 0.83266429\n",
      " 0.83861476 0.84049152 0.83080794        nan 0.83272057 0.85378376\n",
      " 0.8633291  0.84238523 0.86369834 0.82583601 0.85197814 0.86993434\n",
      " 0.87852387 0.8618266 ]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"precision\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = {\n",
    "    'min_samples_split': np.arange(1,80),  \n",
    "    'min_samples_leaf': np.arange(1,40),\n",
    "    'min_impurity_decrease': np.arange(0.0001, 0.01, 0.0005),\n",
    "    'max_leaf_nodes': np.arange(5, 200), \n",
    "    'max_depth': np.arange(1,50), \n",
    "    'criterion': ['entropy', 'gini'],\n",
    "}\n",
    "\n",
    "dtree = DecisionTreeClassifier()\n",
    "rand_search = RandomizedSearchCV(estimator = dtree, param_distributions=param_grid, cv=kfolds, n_iter=500,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = rand_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {rand_search.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search.best_params_}\")\n",
    "\n",
    "bestRecallTree = rand_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cf416f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_matrix = confusion_matrix(y_test, rand_search.predict(X_test))\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "performance = pd.concat([performance, pd.DataFrame({'model':\"Random search\", \n",
    "                                                    'Accuracy': [(TP+TN)/(TP+TN+FP+FN)], \n",
    "                                                    'Precision': [TP/(TP+FP)], \n",
    "                                                    'Recall': [TP/(TP+FN)], \n",
    "                                                    'F1': [2*TP/(2*TP+FP+FN)]\n",
    "                                                     }, index=[0])])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c15df26",
   "metadata": {},
   "source": [
    "# 5. Decision tree model using the gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5468e2e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2268 candidates, totalling 11340 fits\n",
      "The best precision score is 0.8450845046005139\n",
      "... with parameters: {'criterion': 'entropy', 'max_depth': 15, 'max_leaf_nodes': 162, 'min_impurity_decrease': 0.0051, 'min_samples_leaf': 5, 'min_samples_split': 11}\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"precision\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = {\n",
    "    'min_samples_split': np.arange(11,14),  \n",
    "    'min_samples_leaf': np.arange(4,7),\n",
    "    'min_impurity_decrease': np.arange(0.0048, 0.0054, 0.0001),\n",
    "    'max_leaf_nodes': np.arange(162,168), \n",
    "    'max_depth': np.arange(15,21), \n",
    "    'criterion': ['entropy'],\n",
    "}\n",
    "\n",
    "dtree = DecisionTreeClassifier()\n",
    "grid_search = GridSearchCV(estimator = dtree, param_grid=param_grid, cv=kfolds, \n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {grid_search.best_score_}\")\n",
    "print(f\"... with parameters: {grid_search.best_params_}\")\n",
    "\n",
    "bestRecallTree = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1a97b366",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_matrix = confusion_matrix(y_test, grid_search.predict(X_test))\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "performance = pd.concat([performance, pd.DataFrame({'model':\"Grid search\", \n",
    "                                                    'Accuracy': [(TP+TN)/(TP+TN+FP+FN)], \n",
    "                                                    'Precision': [TP/(TP+FP)], \n",
    "                                                    'Recall': [TP/(TP+FN)], \n",
    "                                                    'F1': [2*TP/(2*TP+FP+FN)]\n",
    "                                                     }, index=[0])])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c72da9",
   "metadata": {},
   "source": [
    "# 6. SVM Polynomial Kernel Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a7edc2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_poly_model = SVC(kernel=\"poly\", degree=3, coef0=1, C=10,probability=True)\n",
    "_ = svm_poly_model.fit(X_train, np.ravel(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "be7955d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_preds = svm_poly_model.predict(X_test)\n",
    "c_matrix = confusion_matrix(y_test, model_preds)\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "performance = pd.concat([performance, pd.DataFrame({'model':\"poly svm\", \n",
    "                                                    'Accuracy': [(TP+TN)/(TP+TN+FP+FN)], \n",
    "                                                    'Precision': [TP/(TP+FP)], \n",
    "                                                    'Recall': [TP/(TP+FN)], \n",
    "                                                    'F1': [2*TP/(2*TP+FP+FN)]\n",
    "                                                     }, index=[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "62524f1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Random search</td>\n",
       "      <td>0.849110</td>\n",
       "      <td>0.853053</td>\n",
       "      <td>0.841808</td>\n",
       "      <td>0.847393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Grid search</td>\n",
       "      <td>0.846298</td>\n",
       "      <td>0.837937</td>\n",
       "      <td>0.856874</td>\n",
       "      <td>0.847300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>poly svm</td>\n",
       "      <td>0.867854</td>\n",
       "      <td>0.855839</td>\n",
       "      <td>0.883239</td>\n",
       "      <td>0.869323</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           model  Accuracy  Precision    Recall        F1\n",
       "0  Random search  0.849110   0.853053  0.841808  0.847393\n",
       "0    Grid search  0.846298   0.837937  0.856874  0.847300\n",
       "0       poly svm  0.867854   0.855839  0.883239  0.869323"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3929b2e4",
   "metadata": {},
   "source": [
    "Using precision as the scoring measure, we can observe that precision for poly svm model stands out to be the highest with 85.5% followed by Random search(85.3%) and Grid search(83.47%). A high precision score is a good sign, as it indicates that the model is accurately predicting positive cases. Considering the additional metrics such as accuracy, recall, F1 score also, we can conclude that poly svm model is the best."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
